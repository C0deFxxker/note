<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/note/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/note/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/note/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/note/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/note/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/note/images/favicon.ico?v=5.1.4">


  <link rel="mask-icon" href="/note/images/favicon.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="本文主要介绍Ceph存储集群的部署方式，以及三种存储类型服务的构建及简单使用，不对内部原理做深入介绍，过后会编写另外的文章聊这个话题。本文属于个人学习笔记，如有错误欢迎指正，部署期间踩到的坑写在了Q&amp;amp;A。">
<meta property="og:type" content="article">
<meta property="og:title" content="Ceph集群部署">
<meta property="og:url" content="http://yoursite.com/2020/02/04/ceph-1/index.html">
<meta property="og:site_name" content="C0deHvb">
<meta property="og:description" content="本文主要介绍Ceph存储集群的部署方式，以及三种存储类型服务的构建及简单使用，不对内部原理做深入介绍，过后会编写另外的文章聊这个话题。本文属于个人学习笔记，如有错误欢迎指正，部署期间踩到的坑写在了Q&amp;amp;A。">
<meta property="og:locale" content="zh">
<meta property="og:image" content="http://yoursite.com/images/ceph.png">
<meta property="og:image" content="https://docs.ceph.com/docs/master/_images/cephfs-architecture.svg">
<meta property="og:image" content="https://docs.ceph.com/docs/master/_images/ditaa-50d12451eb76c5c72c4574b08f0320b39a42e5f1.png">
<meta property="og:image" content="https://docs.ceph.com/docs/master/_images/ditaa-dc9f80d771b55f2daa5cbbfdb2dd0d3e6dfc17c0.png">
<meta property="og:updated_time" content="2020-02-04T02:40:09.638Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ceph集群部署">
<meta name="twitter:description" content="本文主要介绍Ceph存储集群的部署方式，以及三种存储类型服务的构建及简单使用，不对内部原理做深入介绍，过后会编写另外的文章聊这个话题。本文属于个人学习笔记，如有错误欢迎指正，部署期间踩到的坑写在了Q&amp;amp;A。">
<meta name="twitter:image" content="http://yoursite.com/images/ceph.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/note/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/02/04/ceph-1/">





  <title>Ceph集群部署 | C0deHvb</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/note/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">C0deHvb</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/note/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/note/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/note/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/note/2020/02/04/ceph-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="黎毅麟">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/note/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="C0deHvb">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Ceph集群部署</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-02-04T10:40:09+08:00">
                2020-02-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/note/categories/Ceph/" itemprop="url" rel="index">
                    <span itemprop="name">Ceph</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文主要介绍Ceph存储集群的部署方式，以及三种存储类型服务的构建及简单使用，不对内部原理做深入介绍，过后会编写另外的文章聊这个话题。本文属于个人学习笔记，如有错误欢迎指正，部署期间踩到的坑写在了Q&amp;A。</p>
<a id="more"></a>

<h1 id="组件介绍"><a href="#组件介绍" class="headerlink" title="组件介绍"></a>组件介绍</h1><p>一个Ceph存储群集至少需要一个监控器(ceph-mon)，管理器(ceph-mgr)和对象存储守护程序(ceph-osd)。运行Ceph文件系统客户端时，还需要元数据服务器(ceph-mds)。</p>
<ul>
<li><strong>管理器(ceph-mon)</strong>: 维护集群状态的映射，包括监视器映射，管理器映射，OSD映射，MDS映射和CRUSH映射。这些映射是Ceph守护程序相互协调所需的关键群集状态。监视器还负责管理守护程序和客户端之间的身份验证。通常至少需要三个监视器才能实现冗余和高可用性。</li>
<li><strong>管理器(ceph-mgr)</strong>: 负责跟踪运行时指标和Ceph集群的当前状态，包括存储利用率，当前性能指标和系统负载。Ceph Manager守护程序基于python模块进行管理并且负责公开Ceph集群信息（包括基于Web的Ceph Dashboard和 REST API）。通常，至少需要两个管理器才能实现高可用性。</li>
<li><strong>对象存储守护程序(ceph-osd)</strong>: 处理数据复制，恢复，重新平衡，并通过检查其他Ceph OSD守护程序的心跳来向Ceph监视器和管理器提供一些监视信息。通常至少需要3个Ceph OSD才能实现冗余和高可用性。</li>
<li><strong>元数据服务器(ceph-mds)</strong>: 代表Ceph文件系统存储元数据（即Ceph块设备和Ceph对象存储不使用MDS）。Ceph的元数据服务器允许POSIX文件系统的用户来执行基本的命令（如 ls，find等等）。</li>
</ul>
<p>Ceph将数据作为对象存储在逻辑存储池中。Ceph 使用 CRUSH算法计算一个对象应该包含在哪个Placement group，并进一步计算这个Placement group应存储在哪个Ceph OSD守护程序。CRUSH算法使Ceph存储集群能够动态扩展，重新平衡和恢复。下面是Ceph文件对象存储流程图：</p>
<p><img src="../../../../images/ceph.png" alt="Ceph文件对象存储流程图"></p>
<h1 id="预安装"><a href="#预安装" class="headerlink" title="预安装"></a>预安装</h1><h2 id="添加密钥"><a href="#添加密钥" class="headerlink" title="添加密钥"></a>添加密钥</h2><h3 id="APT"><a href="#APT" class="headerlink" title="APT"></a>APT</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -q -O- <span class="string">'https://download.ceph.com/keys/release.asc'</span> | sudo apt-key add -</span><br></pre></td></tr></table></figure>

<h3 id="RPM"><a href="#RPM" class="headerlink" title="RPM"></a>RPM</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rpm --import <span class="string">'https://download.ceph.com/keys/release.asc'</span></span><br></pre></td></tr></table></figure>

<h2 id="添加Ceph源"><a href="#添加Ceph源" class="headerlink" title="添加Ceph源"></a>添加Ceph源</h2><h3 id="Ubuntu或Debian"><a href="#Ubuntu或Debian" class="headerlink" title="Ubuntu或Debian"></a>Ubuntu或Debian</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-add-repository <span class="string">'deb https://download.ceph.com/debian-&#123;ceph-release&#125;/ $(lsb_release -sc) main'</span></span><br><span class="line">sudo apt update</span><br></pre></td></tr></table></figure>

<h3 id="RHEL或CentOS"><a href="#RHEL或CentOS" class="headerlink" title="RHEL或CentOS"></a>RHEL或CentOS</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加入Ceph yum源文件</span></span><br><span class="line">$ cat  /etc/yum.repos.d/ceph.repo</span><br><span class="line">[ceph]</span><br><span class="line">name=Ceph packages <span class="keyword">for</span> <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://download.ceph.com/rpm-&#123;ceph-release&#125;/&#123;distro&#125;/<span class="variable">$basearch</span></span><br><span class="line">enabled=1</span><br><span class="line">priority=2</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=https://download.ceph.com/rpm-&#123;ceph-release&#125;/&#123;distro&#125;/noarch</span><br><span class="line">enabled=1</span><br><span class="line">priority=2</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph <span class="built_in">source</span> packages</span><br><span class="line">baseurl=https://download.ceph.com/rpm-&#123;ceph-release&#125;/&#123;distro&#125;/SRPMS</span><br><span class="line">enabled=0</span><br><span class="line">priority=2</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br><span class="line"></span><br><span class="line">$ yum clean all</span><br><span class="line">$ yum makecache</span><br></pre></td></tr></table></figure>

<h2 id="时间同步"><a href="#时间同步" class="headerlink" title="时间同步"></a>时间同步</h2><p>需要确保Ceph集群中所有节点的时间同步，使用 ntp 工具做时间同步。<br>CentOS / RHEL:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install ntp ntpdate ntp-doc</span><br></pre></td></tr></table></figure>

<p>Debian / Ubuntu:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ntpsec</span><br></pre></td></tr></table></figure>

<p>或:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install chrony</span><br></pre></td></tr></table></figure>

<h2 id="安装SSH服务"><a href="#安装SSH服务" class="headerlink" title="安装SSH服务"></a>安装SSH服务</h2><p>Ceph集群部署是在总控机器上通过SSH连接其它集群节点进行安装，所以需要保证所有Ceph集群节点都开启了SSH服务。</p>
<p>CentOS / RHEL:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install openssh-server</span><br></pre></td></tr></table></figure>

<p>Debian / Ubuntu:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install openssh-server</span><br></pre></td></tr></table></figure>

<h2 id="创建Ceph部署用户"><a href="#创建Ceph部署用户" class="headerlink" title="创建Ceph部署用户"></a>创建Ceph部署用户</h2><p>为了安全起见不建议使用root用户进行部署，创建一个Ceph部署专用的用户更为合理，而且需要保证部署机可通过“SSH免密登录”的方式使用Ceph部署用户登录到集群中的各个节点。</p>
<p>创建用户:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh user@ceph-server</span><br><span class="line">sudo useradd -d /home/&#123;username&#125; -m &#123;username&#125;</span><br><span class="line">sudo passwd &#123;username&#125;</span><br></pre></td></tr></table></figure>

<p>需要保证Ceph部署用户是一个 sudoer:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"&#123;username&#125; ALL = (root) NOPASSWD:ALL"</span> | sudo tee /etc/sudoers.d/&#123;username&#125;</span><br><span class="line">sudo chmod 0440 /etc/sudoers.d/&#123;username&#125;</span><br></pre></td></tr></table></figure>

<h2 id="免密登录"><a href="#免密登录" class="headerlink" title="免密登录"></a>免密登录</h2><p>在主节点上执行下列指令实现免密登录：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-copy-id ceph@&#123;host&#125;</span><br></pre></td></tr></table></figure>

<h2 id="安装ceph-deploy"><a href="#安装ceph-deploy" class="headerlink" title="安装ceph-deploy"></a>安装ceph-deploy</h2><h3 id="Ubuntu或Debian-1"><a href="#Ubuntu或Debian-1" class="headerlink" title="Ubuntu或Debian"></a>Ubuntu或Debian</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install ceph-deploy</span><br></pre></td></tr></table></figure>

<h3 id="RHEL或CentOS-1"><a href="#RHEL或CentOS-1" class="headerlink" title="RHEL或CentOS"></a>RHEL或CentOS</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install ceph-deploy</span><br></pre></td></tr></table></figure>

<h1 id="部署Ceph集群"><a href="#部署Ceph集群" class="headerlink" title="部署Ceph集群"></a>部署Ceph集群</h1><p> 官方提供了三种部署Ceph集群的方法，分别是：ceph-deploy, cephadm, 手动安装。这里只介绍最大众化的安装方式： ceph-deploy。</p>
<p> 演示案例采用3节点集群部署，主机名与IP映射如下：</p>
<ul>
<li>node1: 192.168.56.101</li>
<li>node2: 192.168.56.102</li>
<li>node3: 192.168.56.103</li>
</ul>
<p>以 node1 作为部署主节点，如没有特殊说明下面给出的指令都运行在node1节点上。</p>
<h2 id="部署-ceph-mon"><a href="#部署-ceph-mon" class="headerlink" title="部署 ceph-mon"></a>部署 ceph-mon</h2><ol>
<li>创建 Ceph 配置文件:<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计划三个节点都部署ceph-mon服务，格式: ceph-deploy new &#123;initial-monitor-node(s)&#125;</span></span><br><span class="line">$ ceph-deploy new node1 node2 node3</span><br><span class="line"><span class="comment"># 创建成功后会在当前目录生成这三个文件，其中ceph.conf是集群配置文件，ceph.mon.keyring是ceph-mon服务的密钥</span></span><br><span class="line">$ ls</span><br><span class="line">ceph.conf ceph-deploy-ceph.log  ceph.mon.keyring</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>一个Ceph集群中至少拥有一个ceph-mon，为了实现高可用，Ceph集群可以部署多个ceph-mon服务，当集群中某一个ceph-mon服务挂掉后也不会影响整个集群使用。Ceph使用Paxos算法，该算法需要大多数ceph-mon（即，大于N / 2，其中N是ceph-mon的数量）才能形成仲裁。监视器的奇数往往会更好，尽管这不是必需的。</p>
<p>这里只是配置初始的ceph-mon节点主机，可以在后续集群正常使用过程中添加新的ceph-mon。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 后续添加ceph-mon的指令格式: ceph-deploy mon add &#123;ceph-nodes&#125;</span></span><br><span class="line">$ ceph-deploy mon add node4 node5</span><br></pre></td></tr></table></figure>

<p>一旦添加了新的Ceph监视器，Ceph将开始同步监视器并形成仲裁。您可以通过执行以下操作检查仲裁状态：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph quorum_status --format json-pretty</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p>如果你的集群节点存在多个网卡，需要为ceph.conf文件添加Ceph集群通信网段的配置。演示网络是192.168.56.0/24网段，所以添加配置如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># public network = &#123;ip-address&#125;/&#123;bits&#125;</span></span><br><span class="line">public network = 192.168.56.0/24</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果采用了IPv6，还需要在ceph.conf中添加如下配置:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ms <span class="built_in">bind</span> ipv6 = <span class="literal">true</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>为每个节点安装 Ceph 依赖库。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy install --no-adjust-repos node1 node2 node3</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>这个指令会通过SSH远程到 node1,node2,node3 三个节点分别执行 <code>yum install</code> 或 <code>apt install</code> 安装，因为前面预处理章节已经配置了ceph的yum源或apt源，<code>--no-adjust-repos</code> 参数表明不修改节点的yum源或apt源，直接执行 <code>yum install</code> 或 <code>apt install</code> 指令下载需要的依赖库，不添加此参数的话可能会造成下载的库版本与我们预期的不一致问题。</p>
<ol start="5">
<li>初始化ceph-mon并创建组件密钥。<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>一旦指令执行成功，在当前目录下将会新增一些keyring文件：</p>
<ul>
<li>ceph.client.admin.keyring</li>
<li>ceph.bootstrap-mgr.keyring</li>
<li>ceph.bootstrap-osd.keyring</li>
<li>ceph.bootstrap-mds.keyring</li>
<li>ceph.bootstrap-rgw.keyring</li>
<li>ceph.bootstrap-rbd.keyring</li>
<li>ceph.bootstrap-rbd-mirror.keyring</li>
</ul>
<ol start="6">
<li>使用<code>ceph-deploy</code>复制配置文件以及admin用户的密钥到Ceph集群的其它节点，这样做其它节点也可以通过命令行对Ceph进行操作。<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以指定多个节点拥有ceph集群的管理权限，格式：ceph-deploy admin &#123;ceph-node(s)&#125;</span></span><br><span class="line">$ ceph-deploy admin node1</span><br><span class="line"><span class="comment"># 确保当前用户拥有ceph.client.admin.keyring文件的读权限</span></span><br><span class="line">$ sudo chown &#123;user&#125;:&#123;group&#125; /etc/ceph/ceph.client.admin.keyring</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>指令执行完毕后，在指定节点的 /etc/ceph 目录下会新增 ceph.client.admin.keyring 密钥文件，ceph集群管理命令需要通过这个密钥认证才能执行。</p>
<p>node1上执行ceph命令，确认admin密钥正确:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[ceph@node1 ~]$ ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     d809fd13-1fd6-4591-bd2b-5d4eabbadaba</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            no active mgr</span><br><span class="line">            OSD count 0 &lt; osd_pool_default_size 3</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum node1,node2,node3 (age 11m)</span><br><span class="line">    mgr: no daemons active (since 63s)</span><br><span class="line">    osd: 0 osds: 0 up, 0 <span class="keyword">in</span></span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>

<p>在没有赋予管理权限的节点上尝试执行Ceph管理指令将会报错：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 node2或node3 上执行</span></span><br><span class="line">[ceph@node2 ~]$ ceph -s</span><br><span class="line">[errno 2] error connecting to the cluster</span><br></pre></td></tr></table></figure>

<h2 id="部署-ceph-mgr"><a href="#部署-ceph-mgr" class="headerlink" title="部署 ceph-mgr"></a>部署 ceph-mgr</h2><p>在 luminous 或更新的版本需要这一步。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy mgr create node1</span><br><span class="line"><span class="comment"># 看到Ceph集群状态中的 no active mgr 警告信息已经消失</span></span><br><span class="line">$ ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     d809fd13-1fd6-4591-bd2b-5d4eabbadaba</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            OSD count 0 &lt; osd_pool_default_size 3</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum node1,node2,node3 (age 14m)</span><br><span class="line">    mgr: node1(active, since 1.52953s)</span><br><span class="line">    osd: 0 osds: 0 up, 0 <span class="keyword">in</span></span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   0 B used, 0 B / 0 B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure>

<h2 id="添加OSD"><a href="#添加OSD" class="headerlink" title="添加OSD"></a>添加OSD</h2><p>OSD是Ceph集群的基础存储单元，需要指定一个裸盘、主分区或逻辑分区作为一个OSD服务。</p>
<p>笔者当前演示的环境是用3台虚拟机并各创建一个虚拟裸盘实现，3个节点均挂载这个虚拟磁盘在/dev/sdb上，大小为30G。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ lsblk</span><br><span class="line">NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda               8:0    0   10G  0 disk </span><br><span class="line">├─sda1            8:1    0    1G  0 part /boot</span><br><span class="line">└─sda2            8:2    0    9G  0 part </span><br><span class="line">  ├─centos-root 253:0    0    8G  0 lvm  /</span><br><span class="line">  └─centos-swap 253:1    0    1G  0 lvm  [SWAP]</span><br><span class="line">sdb               8:16   0   30G  0 disk </span><br><span class="line">sr0              11:0    1 1024M  0 rom</span><br></pre></td></tr></table></figure>

<p>使用<code>ceph-deploy</code>添加OSD指令如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 格式: ceph-deploy osd create --data &#123;device&#125; &#123;ceph-node&#125;</span></span><br><span class="line"><span class="comment"># 如果你使用的是逻辑分区，参数格式为 --data vg_group/lv_name</span></span><br><span class="line">$ ceph-deploy osd create --data /dev/sdb node1</span><br><span class="line">$ ceph-deploy osd create --data /dev/sdb node2</span><br><span class="line">$ ceph-deploy osd create --data /dev/sdb node3</span><br></pre></td></tr></table></figure>

<p>这里可能因Ceph版本不同细节有些许区别，若不幸遇到报错需要根据日志自己解决。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署完毕后再次查看集群状态可以看到3个OSD已经启动</span></span><br><span class="line">$ ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     d809fd13-1fd6-4591-bd2b-5d4eabbadaba</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum node1,node2,node3 (age 34m)</span><br><span class="line">    mgr: node1(active, since 19m)</span><br><span class="line">    osd: 3 osds: 3 up (since 5s), 3 <span class="keyword">in</span> (since 5s)</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0 B</span><br><span class="line">    usage:   3.0 GiB used, 84 GiB / 87 GiB avail</span><br><span class="line">    pgs:</span><br><span class="line"><span class="comment"># 查看OSD树状结构</span></span><br><span class="line">$ ceph osd tree</span><br><span class="line">ID CLASS WEIGHT  TYPE NAME      STATUS REWEIGHT PRI-AFF </span><br><span class="line">-1       0.08487 root default                           </span><br><span class="line">-3       0.02829     host node1                         </span><br><span class="line"> 0   hdd 0.02829         osd.0      up  1.00000 1.00000 </span><br><span class="line">-5       0.02829     host node2                         </span><br><span class="line"> 1   hdd 0.02829         osd.1      up  1.00000 1.00000 </span><br><span class="line">-7       0.02829     host node3                         </span><br><span class="line"> 2   hdd 0.02829         osd.2      up  1.00000 1.00000</span><br></pre></td></tr></table></figure>

<h2 id="删除OSD"><a href="#删除OSD" class="headerlink" title="删除OSD"></a>删除OSD</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从crush中移除节点</span></span><br><span class="line">$ ceph osd crush remove osd.<span class="variable">$ID</span></span><br><span class="line"><span class="comment"># 将节点服务停止</span></span><br><span class="line">$ ceph osd down osd.<span class="variable">$ID</span></span><br><span class="line"><span class="comment"># 删除节点</span></span><br><span class="line">$ ceph osd rm osd.<span class="variable">$ID</span></span><br><span class="line"><span class="comment"># 删除节点认证</span></span><br><span class="line">$ ceph auth del osd.<span class="variable">$ID</span></span><br></pre></td></tr></table></figure>

<h2 id="添加MDS"><a href="#添加MDS" class="headerlink" title="添加MDS"></a>添加MDS</h2><p>ceph-mds是CephFS必要的服务，如果可以采用部署<a href="https://docs.ceph.com/docs/master/mgr/orchestrator_cli/" target="_blank" rel="noopener">Orchestrator</a>自动为您的文件系统创建并配置MDS（请参阅 <a href="https://docs.ceph.com/docs/master/mgr/orchestrator_cli/#current-implementation-status" target="_blank" rel="noopener">Orchestrator部署表</a>）。否则，参照下列方法手动部署mds。</p>
<p>想要启动CephFS前必须启动ceph-mds服务，执行下列脚本可部署mds服务（需要先配置环境变量）：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建mds存储目录。</span></span><br><span class="line">$ mkdir -p /var/lib/ceph/mds/<span class="variable">$CLUSTER_NAME</span>-<span class="variable">$MDS_ID</span></span><br><span class="line"><span class="comment"># 部署ceph-mds服务，命令格式: ceph-deploy mds create &#123;ceph-node&#125;</span></span><br><span class="line">$ ceph-deploy mds create node1</span><br><span class="line"><span class="comment"># 检测mds是否启动成功（"1 up:standby" 表示有一个mds节点启动成功）</span></span><br><span class="line">$ ceph mds <span class="built_in">stat</span></span><br><span class="line"> 1 up:standby</span><br></pre></td></tr></table></figure>

<p>至此Ceph集群基础组件部署完毕，可以根据需要往下部署不同存储类型的服务。</p>
<h1 id="三种Ceph存储类型的部署"><a href="#三种Ceph存储类型的部署" class="headerlink" title="三种Ceph存储类型的部署"></a>三种Ceph存储类型的部署</h1><p>CephFS致力于为各种应用程序提供最新，多用途，高可用性和高性能的文件存储。提供了三种存储类型：</p>
<ul>
<li>CephFS</li>
<li>Ceph Object Storage</li>
<li>CephRBD</li>
</ul>
<p>避免篇幅过长，本文只介绍最简单的部署及使用方式，过后会有其它文章详述各种存储类型的运行机制。</p>
<h2 id="CephFS"><a href="#CephFS" class="headerlink" title="CephFS"></a>CephFS</h2><h3 id="服务端部署"><a href="#服务端部署" class="headerlink" title="服务端部署"></a>服务端部署</h3><p>CephFS 是 Ceph File System 的简称，它是在Ceph的分布式对象存储RADOS之上构建的POSIX兼容文件系统。<br>文件元数据与文件数据存储在单独的RADOS池中，并通过可调整大小的元数据服务器或MDS集群提供服务，该集群是可拓展的以支持高吞吐量的元数据工作负载。</p>
<p>文件系统的客户端可以直接访问RADOS以读取和写入文件数据块，因此，工作负载可能会随着基础RADOS对象存储的数量线性扩展，也就是说，没有网关或代理为客户端中介数据I / O。</p>
<p>通过MDS集群协调对数据的访问，每个MDS都会将对元数据的变化汇总到日志以对RADOS进行一系列有效写入。MDS不会在本地存储任何元数据状态。此模型允许在POSIX文件系统的上下文中客户端之间进行连贯且快速的协作。</p>
<p><img src="https://docs.ceph.com/docs/master/_images/cephfs-architecture.svg" alt="CephFS架构图"></p>
<p>创建cephfs可以直接使用如下指令:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建名为 testfs 的文件系统资源</span></span><br><span class="line">$ ceph fs volume create testfs</span><br><span class="line"><span class="comment"># 查看已有文件系统资源</span></span><br><span class="line">$ ceph fs ls</span><br><span class="line">name: testfs, metadata pool: cephfs.testfs.meta, data pools: [cephfs.testfs.data ]</span><br><span class="line"><span class="comment"># ceph-mds 管理了这个FS</span></span><br><span class="line">$ ceph mds <span class="built_in">stat</span></span><br><span class="line">testfs:1 &#123;0=node1=up:active&#125;</span><br><span class="line"><span class="comment"># 查看集群状态</span></span><br><span class="line">$ ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     908baedd-c2d1-4977-8aa8-dea83b31d271</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum node1 (age 42m)</span><br><span class="line">    mgr: node1(active, since 50m)</span><br><span class="line">    mds: testfs:1 &#123;0=node1=up:active&#125;</span><br><span class="line">    osd: 3 osds: 3 up (since 25m), 3 <span class="keyword">in</span> (since 51m)</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   2 pools, 96 pgs</span><br><span class="line">    objects: 24 objects, 3.3 KiB</span><br><span class="line">    usage:   3.0 GiB used, 27 GiB / 30 GiB avail</span><br><span class="line">    pgs:     96 active+clean</span><br></pre></td></tr></table></figure>

<p>该指令将自动创建一个元数据存储池（<code>cephfs.&lt;FSNAME&gt;.meta</code>）与一个内容存储池（<code>cephfs.&lt;FSNAME&gt;.data</code>）。</p>
<p>若不幸出现了警告内容为<code>too few PGs per OSD (n &lt; min 30)</code>，请参考Q&amp;A章节获知解决方案。</p>
<h3 id="客户端挂载"><a href="#客户端挂载" class="headerlink" title="客户端挂载"></a>客户端挂载</h3><p>CephFS挂载有两种方法：使用Linux内核驱动或是使用ceph-fuse工具。实际上比较常用的是ceph-fuse工具方法，所以本文只介绍这种挂载方式。</p>
<ol>
<li><p>需要先安装 ceph-fuse</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y ceph-fuse</span><br></pre></td></tr></table></figure>
</li>
<li><p>在客户机生成服务端路径相关配置文件。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在客户机上执行</span></span><br><span class="line">$ mkdir -p -m 755 /etc/ceph</span><br><span class="line">$ ssh &#123;user&#125;@&#123;mon-host&#125; <span class="string">"sudo ceph config generate-minimal-conf"</span> | sudo tee /etc/ceph/ceph.conf</span><br><span class="line"><span class="comment"># 确保文件可以被其它用户访问到</span></span><br><span class="line">$ chmod 644 /etc/ceph/ceph.conf</span><br><span class="line"><span class="comment"># 文件中记录了ceph-mon的地址以及集群ID</span></span><br><span class="line">$ cat /etc/ceph/ceph.conf</span><br><span class="line">[global]</span><br><span class="line">fsid = 908baedd-c2d1-4977-8aa8-dea83b31d271</span><br><span class="line">mon_host = [v2:192.168.56.101:3300/0,v1:192.168.56.101:6789/0]</span><br></pre></td></tr></table></figure>
</li>
<li><p>因为服务端采用了CephX认证方式，需要为客户端机器生成一份对应的密钥文件。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个foo用户授权名为 testfs 的CephFS资源，并且拥有根路径下( / )的读写权限( rw )</span></span><br><span class="line">$ ssh &#123;user&#125;@&#123;mon-host&#125; <span class="string">"sudo ceph fs authorize testfs client.foo / rw"</span> | sudo tee /etc/ceph/ceph.client.foo.keyring</span><br><span class="line">$ chmod 600 /etc/ceph/ceph.client.foo.keyring</span><br></pre></td></tr></table></figure>
</li>
<li><p>挂载CephFS到指定客户机目录</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir /mnt/mycephfs</span><br><span class="line">$ sudo ceph-fuse --id foo /mnt/mycephfs</span><br><span class="line">ceph-fuse[4131]: starting ceph client</span><br><span class="line">2020-02-02 05:21:21.476 7f5f9b493e00 -1 init, newargv = 0x561161876670 newargc=7</span><br><span class="line">ceph-fuse[4131]: starting fuse</span><br><span class="line"><span class="comment"># 确认目录挂载成功</span></span><br><span class="line">$ df -h | grep ceph-fuse</span><br><span class="line">ceph-fuse                 26G     0   26G    0% /mnt/mycephfs</span><br></pre></td></tr></table></figure>
</li>
<li><p>撤销挂载卷</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ umount /mnt/mycephfs</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>执行完上述4步后就算是挂载完毕了，但是客户机重启后挂载卷将会消失需要重新执行<code>ceph-fuse</code>指令挂载回来，可以修改<code>/etc/fstab</code>文件做到开机自动挂载，需要在<code>/etc/fstab</code>文件中添加如下配置：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DEVICE PATH       TYPE      OPTIONS</span></span><br><span class="line">none    /mnt/mycephfs  fuse.ceph ceph.id=&#123;user-ID&#125;[,ceph.conf=&#123;path/to/conf.conf&#125;],_netdev,defaults  0 0</span><br></pre></td></tr></table></figure>

<p>例子:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">none    /mnt/mycephfs  fuse.ceph ceph.id=foo,_netdev,defaults  0 0</span><br><span class="line"><span class="comment"># 面向多个Ceph集群时，需要手动指定特定集群的配置文件（默认使用 /etc/ceph/ceph.conf）</span></span><br><span class="line">none    /mnt/mycephfs  fuse.ceph ceph.id=foo,ceph.conf=/etc/ceph/foo.conf,_netdev,defaults  0 0</span><br></pre></td></tr></table></figure>

<h2 id="Ceph-Object-Storage"><a href="#Ceph-Object-Storage" class="headerlink" title="Ceph Object Storage"></a>Ceph Object Storage</h2><p>Ceph对象网关是一个对象存储接口，建立在该对象之上， librados为应用程序提供了通往Ceph存储集群的RESTful网关。</p>
<p>Ceph对象存储支持两个接口：</p>
<ul>
<li>与S3兼容：为对象存储功能提供与Amazon S3 RESTful API的大部分子集兼容的接口。</li>
<li>兼容Swift：为对象存储功能提供与OpenStack Swift API的大部分子集兼容的接口。</li>
</ul>
<p>S3和Swift API共享一个公共的名称空间，因此您可以使用一个API编写数据，而使用另一个API检索数据。</p>
<p><img src="https://docs.ceph.com/docs/master/_images/ditaa-50d12451eb76c5c72c4574b08f0320b39a42e5f1.png" alt="Ceph Object Storage 架构图"></p>
<ol>
<li><p>安装Rados Gateway的依赖库。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ceph-deploy install --rgw &lt;gateway-node1&gt; [&lt;gateway-node2&gt; ...]</span></span><br><span class="line">$ ceph-deploy install --rgw node1</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建Rados网关实例。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ceph-deploy rgw create &lt;gateway-node1&gt;</span></span><br><span class="line">$ ceph-deploy rgw create node1</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>默认Rados网关占用7480端口，可以使用浏览器访问<code>http://node1:7480/</code>得到如下响应，说明网关部署已完毕：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ListAllMyBucketsResult</span> <span class="attr">xmlns</span>=<span class="string">"http://s3.amazonaws.com/doc/2006-03-01/"</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">Owner</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">ID</span>&gt;</span>anonymous<span class="tag">&lt;/<span class="name">ID</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">DisplayName</span>&gt;</span><span class="tag">&lt;/<span class="name">DisplayName</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">Owner</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">Buckets</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">Buckets</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ListAllMyBucketsResult</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>[可选] 修改Rados网关端口。修改<code>ceph.conf</code>文件添加如下内容:<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[client.rgw.node1]  <span class="comment"># node1对应上面第2步部署的节点</span></span><br><span class="line">rgw_frontends = <span class="string">"civetweb port=80"</span>  <span class="comment"># 修改为80端口</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>然后使用ceph-deploy更新相关节点的<code>ceph.conf</code>文件:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ceph-deploy --overwrite-conf config push &lt;gateway-node&gt; [&lt;other-nodes&gt;]</span></span><br><span class="line">$ ceph-deploy --overwrite-conf config push node1</span><br><span class="line"><span class="comment"># 重启rados网关服务才能使配置生效</span></span><br><span class="line">$ sudo systemctl restart ceph-radosgw.service</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>[可选] 使Rados网关支持SSL协议。需要先生成证书（略），然后修改<code>ceph.conf</code>文件添加如下配置：<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[client.rgw.node1]</span><br><span class="line">rgw_frontends = civetweb port=443s ssl_certificate=/etc/ceph/private/keyandcert.pem <span class="comment"># ssl_certificate指定证书路径</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>也可以同时开放HTTP协议端口与HTTPS协议端口，配置修改如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[client.rgw.node1]</span><br><span class="line">rgw_frontends = civetweb port=80+443s ssl_certificate=/etc/ceph/private/keyandcert.pem</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>添加用户。Ceph Object Storage 的 Rados 网关支持 S3 与 Swift 两种 API，但它们的用户权限管理有所不同，所以需要根据需要生成不同类型的用户。</li>
</ol>
<ul>
<li>添加S3用户: <code>sudo radosgw-admin user create --uid=&quot;testuser&quot; --display-name=&quot;First User&quot;</code></li>
<li>添加Swift用户: <code>sudo radosgw-admin subuser create --uid=testuser --subuser=testuser:swift --access=full</code></li>
</ul>
<p>到此Rados网关基础部署算是完成了，测试网关上传下载功能请参考: <a href="https://docs.ceph.com/docs/master/install/install-ceph-gateway/#access-verification。" target="_blank" rel="noopener">https://docs.ceph.com/docs/master/install/install-ceph-gateway/#access-verification。</a></p>
<h2 id="CephRBD"><a href="#CephRBD" class="headerlink" title="CephRBD"></a>CephRBD</h2><p>块是字节序列（例如，一个512字节的数据块）。基于块的存储接口是使用旋转介质（例如硬盘，CD，软盘甚至传统的9轨磁带）存储数据的最常用方法。块设备接口无处不在使虚拟块设备成为与海量数据存储系统（如Ceph）进行交互的理想选择。</p>
<p>Ceph块设备经过精简配置，可调整大小，并在Ceph集群中的多个OSD上存储条带化数据。Ceph块设备利用了 RADOS功能，例如快照，复制和一致性。Ceph的 RADOS块设备（RBD）使用内核模块或librbd库与OSD进行交互。</p>
<p><img src="https://docs.ceph.com/docs/master/_images/ditaa-dc9f80d771b55f2daa5cbbfdb2dd0d3e6dfc17c0.png" alt="Ceph块设备架构图"></p>
<ol>
<li><p>创建一个用于RBD的存储池。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool create rbdpool</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用RBD工具初始化这个存储池。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rbd pool init rbdpool</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建可以访问这个RBD的用户。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ceph auth get-or-create client.&#123;ID&#125; mon 'profile rbd' osd 'profile &#123;profile name&#125; [pool=&#123;pool-name&#125;][, profile ...]' mgr 'profile rbd [pool=&#123;pool-name&#125;]'</span></span><br><span class="line">$ ceph auth get-or-create client.testrbd mon <span class="string">'profile rbd'</span> osd <span class="string">'profile rbd pool=rbdpool'</span> mgr <span class="string">'profile rbd'</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建块设备镜像。在您开始将数据保存到它们之前，它们实际上并不使用任何物理存储。但是，它们确实具有您使用<code>--size</code>选项设置的最大容量。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建名为testrbd的镜像，大小为1GB。指令格式：rbd create --size &#123;megabytes&#125; &#123;pool-name&#125;/&#123;image-name&#125;</span></span><br><span class="line">$ rbd create --size 1024 rbdpool/testrbd</span><br><span class="line"><span class="comment"># 查看rbdpool存储池中有哪些RBD镜像</span></span><br><span class="line">$ rbd ls rbdpool</span><br><span class="line">testrbd</span><br><span class="line"><span class="comment"># 检索RBD镜像信息: rbd info &#123;pool-name&#125;/&#123;image-name&#125;</span></span><br><span class="line">$ rbd info rbdpool/testrbd</span><br><span class="line">rbd image <span class="string">'testrbd'</span>:</span><br><span class="line">	size 1 GiB <span class="keyword">in</span> 256 objects</span><br><span class="line">	order 22 (4 MiB objects)</span><br><span class="line">	snapshot_count: 0</span><br><span class="line">	id: 56bc9dc78ddfe</span><br><span class="line">	block_name_prefix: rbd_data.56bc9dc78ddfe</span><br><span class="line">	format: 2</span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">	op_features: </span><br><span class="line">	flags: </span><br><span class="line">	create_timestamp: Mon Feb  3 18:56:49 2020</span><br><span class="line">	access_timestamp: Mon Feb  3 18:56:49 2020</span><br><span class="line">	modify_timestamp: Mon Feb  3 18:56:49 2020</span><br><span class="line"><span class="comment"># 调整块设备映像的大小为2GB</span></span><br><span class="line">$  rbd resize --size 2048 rbdpool/testrbd</span><br><span class="line"><span class="comment"># 删除块设备的镜像</span></span><br><span class="line"><span class="comment"># rbd rm &#123;pool-name&#125;/&#123;image-name&#125;</span></span><br><span class="line"><span class="comment"># 例如: rbd rm foo</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>使用librbd进行存储。为了演示方便，这里使用python编写一个简单例子。ceph的python库是在安装ceph时一并安装好的，不需要再执行<code>pip install</code>进行安装。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> rbd</span><br><span class="line"><span class="keyword">import</span> rados</span><br><span class="line"></span><br><span class="line">cluster = rados.Rados(conffile=<span class="string">'/etc/ceph/ceph.conf'</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    cluster.connect()</span><br><span class="line">    print(<span class="string">"Cluster connect success"</span>)</span><br><span class="line">    ioctx = cluster.open_ioctx(<span class="string">'rbdpool'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        image = rbd.Image(ioctx, <span class="string">'testrbd'</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 块设备写入数据</span></span><br><span class="line">            print(<span class="string">"Writing content..."</span>)</span><br><span class="line">            data = <span class="string">'foo'</span> * <span class="number">200</span></span><br><span class="line">            image.write(data, <span class="number">0</span>)</span><br><span class="line">            image.flush()</span><br><span class="line">            print(<span class="string">"Write finished!"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 块设备读取数据</span></span><br><span class="line">            size = image.size()</span><br><span class="line">            print(<span class="string">"Reading content..."</span>)</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            bufsize = <span class="number">1024</span></span><br><span class="line">            buffer = bytes()</span><br><span class="line">            <span class="keyword">while</span> i &lt; size:</span><br><span class="line">                buffer = buffer + image.read(i, min(i + bufsize, size - <span class="number">1</span>))</span><br><span class="line">                i += size</span><br><span class="line">            print(<span class="string">"%s"</span> % str(buffer))</span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">            image.close()</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        ioctx.close()</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    cluster.shutdown()</span><br></pre></td></tr></table></figure>

<h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><h2 id="删除pool报错-pool-deletion-is-disabled"><a href="#删除pool报错-pool-deletion-is-disabled" class="headerlink" title="删除pool报错 pool deletion is disabled"></a>删除pool报错 pool deletion is disabled</h2><p>修改mon节点的配置文件：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure>

<p>添加如下配置内容:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mon]</span><br><span class="line">mon allow pool delete = true</span><br></pre></td></tr></table></figure>

<p>重启ceph-mon服务:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl restart ceph-mon.target</span><br></pre></td></tr></table></figure>

<p>执行删除pool命令:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool delete ecpool ecpool --yes-i-really-really-mean-it</span><br><span class="line">pool <span class="string">'ecpool'</span> removed</span><br></pre></td></tr></table></figure>

<h2 id="100-000-pgs-not-active-undersized-peered"><a href="#100-000-pgs-not-active-undersized-peered" class="headerlink" title="100.000% pgs not active undersized+peered"></a>100.000% pgs not active undersized+peered</h2><p>按照官方文档的说法是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">undersized</span><br><span class="line">The placement group has fewer copies than the configured pool replication level.</span><br></pre></td></tr></table></figure>

<p>意思是这个PG的实际副本数少于配置的副本数。所以这个警告原因是有某个pool的size值大于Ceph OSD集群的节点数了，这样使得Ceph找不到更多的节点数来备份这个PG，PG就会变成undersized状态。</p>
<p>前面的 100.000% 是因为我当时的集群只有一个pool而且就这个pool的size值配错了，所以是100%的PG数都变成undersized状态。这个百分比可能不是100%，实际情况下需要查询各个pool配置情况来找出这个错误配置的pool。</p>
<h2 id="no-active-mgr"><a href="#no-active-mgr" class="headerlink" title="no active mgr"></a>no active mgr</h2><p>集群没有开启ceph-mgr组件，请参照”添加ceph-mgr”章节。</p>
<h2 id="too-few-PGs-per-OSD-n-lt-30"><a href="#too-few-PGs-per-OSD-n-lt-30" class="headerlink" title="too few PGs per OSD ( n &lt; 30 )"></a>too few PGs per OSD ( n &lt; 30 )</h2><p>出现这个警告信息的原因是所有池的PG总数除于OSD数得到的值小于30导致的，可以通过<code>ceph osd pool set $POOL_NAME pg_num $PG_NUM</code>命令修改各个池的PG数，使得总PG数满足如下公式即可：</p>
<p>Total PGs = ((Total_number_of_OSD * 30) / max_replication_count) / pool_count </p>
<p>结算的结果往上取靠近2的N次方的值。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">黎毅麟</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/note/archives/">
              
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#组件介绍"><span class="nav-number">1.</span> <span class="nav-text">组件介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#预安装"><span class="nav-number">2.</span> <span class="nav-text">预安装</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#添加密钥"><span class="nav-number">2.1.</span> <span class="nav-text">添加密钥</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#APT"><span class="nav-number">2.1.1.</span> <span class="nav-text">APT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RPM"><span class="nav-number">2.1.2.</span> <span class="nav-text">RPM</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#添加Ceph源"><span class="nav-number">2.2.</span> <span class="nav-text">添加Ceph源</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ubuntu或Debian"><span class="nav-number">2.2.1.</span> <span class="nav-text">Ubuntu或Debian</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RHEL或CentOS"><span class="nav-number">2.2.2.</span> <span class="nav-text">RHEL或CentOS</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#时间同步"><span class="nav-number">2.3.</span> <span class="nav-text">时间同步</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装SSH服务"><span class="nav-number">2.4.</span> <span class="nav-text">安装SSH服务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建Ceph部署用户"><span class="nav-number">2.5.</span> <span class="nav-text">创建Ceph部署用户</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#免密登录"><span class="nav-number">2.6.</span> <span class="nav-text">免密登录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装ceph-deploy"><span class="nav-number">2.7.</span> <span class="nav-text">安装ceph-deploy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ubuntu或Debian-1"><span class="nav-number">2.7.1.</span> <span class="nav-text">Ubuntu或Debian</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RHEL或CentOS-1"><span class="nav-number">2.7.2.</span> <span class="nav-text">RHEL或CentOS</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#部署Ceph集群"><span class="nav-number">3.</span> <span class="nav-text">部署Ceph集群</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#部署-ceph-mon"><span class="nav-number">3.1.</span> <span class="nav-text">部署 ceph-mon</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#部署-ceph-mgr"><span class="nav-number">3.2.</span> <span class="nav-text">部署 ceph-mgr</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#添加OSD"><span class="nav-number">3.3.</span> <span class="nav-text">添加OSD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#删除OSD"><span class="nav-number">3.4.</span> <span class="nav-text">删除OSD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#添加MDS"><span class="nav-number">3.5.</span> <span class="nav-text">添加MDS</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#三种Ceph存储类型的部署"><span class="nav-number">4.</span> <span class="nav-text">三种Ceph存储类型的部署</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CephFS"><span class="nav-number">4.1.</span> <span class="nav-text">CephFS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#服务端部署"><span class="nav-number">4.1.1.</span> <span class="nav-text">服务端部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#客户端挂载"><span class="nav-number">4.1.2.</span> <span class="nav-text">客户端挂载</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-Object-Storage"><span class="nav-number">4.2.</span> <span class="nav-text">Ceph Object Storage</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CephRBD"><span class="nav-number">4.3.</span> <span class="nav-text">CephRBD</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Q-amp-A"><span class="nav-number">5.</span> <span class="nav-text">Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#删除pool报错-pool-deletion-is-disabled"><span class="nav-number">5.1.</span> <span class="nav-text">删除pool报错 pool deletion is disabled</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#100-000-pgs-not-active-undersized-peered"><span class="nav-number">5.2.</span> <span class="nav-text">100.000% pgs not active undersized+peered</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#no-active-mgr"><span class="nav-number">5.3.</span> <span class="nav-text">no active mgr</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#too-few-PGs-per-OSD-n-lt-30"><span class="nav-number">5.4.</span> <span class="nav-text">too few PGs per OSD ( n &lt; 30 )</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">黎毅麟</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/note/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/note/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/note/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/note/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/note/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/note/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/note/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/note/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/note/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/note/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/note/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  

</body>
</html>
