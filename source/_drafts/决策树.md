<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

# 一、决策树概要
日常生活中，我们脑海中对一个物体进行分类会用一大堆条件一步步地判断来得出答案，比如要判别一个动物是鸡，还是鹅，还是鸭，或三者都不是。我们会通过一些体征一步步地进行判断，例如：  
1. 先判断该动物是否有翅膀。如果没有，则三者（鸡、鹅、鸭）都不是  
2. 判断嘴巴是否扁平。如果不是，则它是鸡  
3. 判断额部是否有橙黄色或黑褐色肉质突起。如果有，则为鹅；否则为鸭  

这样一步步的条件判断，最终分类出该动物物种的方法，我们可以通过决策树来完成，下面给出该例子的图示：

                           _____________有翅膀____________
                       有  |                            |  否
                ________嘴巴扁平________             *不是鸡、鹅、鸭*
            是  |                     | 否
        有橙黄色或黑褐色肉质突起          *鸡*
     是 |                  | 否
       *鹅*               *鸭*

人类脑海中这样的条件判断其实都是基于对一系列特征数据的学习而得出的结论。决策树就是根据这个原理，从一大堆特征数据以及其对应的分类结果中进行学习并自动求得一棵决策树，过后再遇到同样的分类问题，就可以直接把决策树上的决策条件拿来使用就可以了。

观察上图中的叶子节点，所有叶子节点都是最终分类结果值，算法通过从树的根节点上根据判断条件往下走，走到最终叶子节点即为分类结果。

## 如何评判决策树的好坏？
下面我们给出几个判断决策树好坏的指标，决策树算法就是针对这些指标建立数学模型，通过数学模型求得最优值，从而得到最优决策方案：

1. 分类不纯。样本根据决策树算法走到了叶子节点进行归类，但依然存在多个不同分类的样本被归类到同一个叶子节点上，这样的决策树是不好的，没办法帮我们起到分类效果。
2. 树的深度太大。一棵决策树的深度太大，说明判断条件过多，这样的分类及其复杂，不易于理解，效率也不高。
3. 节点分支太多。为了使得分类够纯净，而对某个节点产生非常多的分支。例如：错误的对ID字段作为特征属性来条件判断，得出了ID=1,2,3...无数多个分支，这样跟没有分类区别不大，并且容易造成训练过拟合。 
4. 决策树泛化能力差。如果某棵决策树在训练集中训练结果非常完美，但放到实际应用中得到的效果不佳，就是泛化能力差。

## 决策树的优缺点

优点：

1. 可以生成可以理解的规则。
2. 计算量相对来说不是很大。
3. 可以处理连续和种类字段。
4. 决策树可以清晰的显示哪些字段比较重要

缺点：

1. 对连续性的字段比较难预测。
2. 对有时间顺序的数据，需要很多预处理的工作。
3. 当类别太多时，错误可能就会增加的比较快。
4. 一般的算法分类的时候，只是根据一个字段来分类。


# 二、预备知识

## 特征
下文开始，笔者所谓特征就是对分类有影响的“对象属性”。比如上述鸡鸭鹅分类例子中，我们会为该对象设计这样的一些属性：

```
class Animal {
	int ID
	bool 有翅膀
	bool 嘴巴扁平
	bool 有橙黄色或黑褐色肉质突起
	int 年龄
	bool 性别
	
	...
}
```
其中“ID、年龄、性别"属性对分类没有起到作用，我们就不选取这些属性作为分类**特征**。

## 熵函数
熵函数标准定义为：

$$
F(P) = - \sum^n_{i=1} p_i \cdot \log p_i
\\\\
s.t \space\space\space\space \sum^n_{i=1} p_i = 1, p_i \ge 0, i=1,2,…,n
$$

当 $p_i=\frac{1}{n}$ 时，熵函数取得最大值，并且n越大熵函数的最大值也随之增大。

## 信息熵
判断分类纯不纯使用“信息熵”进行判断。

设实际分类总数为*N*，$P(Y|C=c_i)=\frac{|S_{c_i}|}{|Y|}$ 表示分类i的样本数占整个样本集总数的比率，其中 $S_{c_i}$ 表示样本集Y中分类为$c_i$的子集。

求取信息熵的函数如下：

$$
\begin{aligned}
H(X) &= - \sum^n_{i=1} P(Y|C=c_i) \cdot \log P(Y|C=c_i) \\\\
&= - \sum^n_{i=1} \frac{|S_{c_i}|}{|Y|} \cdot \log \frac{|S_{c_i}|}{|Y|}
\end{aligned}
$$

当样本集中仅有一种分类的样本时，$H(X)=0$。  
当样本集中混杂的样本分类数很多，且每个分类的样本数量越平均，H(X)的值越大。即H(X)越大，分类越不纯。

由此，信息熵就作为决策树中节点分类好坏的一个关键评判因素。

## 条件熵
假设有随机变量(X,Y)，其联合概率分布为: $P(Y|X=x_i)=\frac{|S^{(x_i)}|}{|Y|}(i=1,2,⋯,m)$

定义 $S^{(x_i)}$ 为样本集Y中特征 $X=x_i$ 的子集，$S_{c_i}^{(x_i)}$ 为 $S_{c_i}$ 中特征 $X=x_i$ 的子集。

则条件熵 $H(Y∣X)$ 表示对给定样本集Y在基于特征X进行分类所得结果的混乱程度：
$$
\begin{aligned}
H(Y|X) &= \sum^m_{i=1} P(Y|X=x_i) \cdot H(Y|X=x_i)\\\\
		&= \sum^m_{i=1} \frac{|S^{(x_i)}|}{|Y|} \cdot H(Y|X=x_i)\\\\
		&= \sum^m_{i=1} \Bigg( \frac{|S^{(x_i)}|}{|Y|} \cdot \bigg( \sum^n_{i=1} \frac{|S_{c_i}^{(x_i)}|}{|S_{x_i}|} \cdot \log \frac{|S_{c_i}^{(x_i)}|}{|S_{x_i}|} \bigg) \Bigg)
\end{aligned}
$$

这个值越小，说明按照特征X进行分类后所得到的结果越纯净。

## 信息增益
**信息增益** 表示对给定样本集Y在基于特征X进行分类后所得结果的混乱程度与原样本集Y的混乱程度的差值。
$$
g(Y,X)=H(Y)-H(Y|X)
$$

通过信息增益，我们可以判断对某个节点Y进行再分类时，应采取信息增益最大的特征X进行分类。

# 三、ID3算法
有了上面的预备知识，接下来讲解ID3算法就非常简单了，和其它机器学习算法一样，ID3算法分为 __训练__ 和 __测试__ 两个过程。

## 训练
ID3的训练过程就是建树过程，其具体步骤如下：

1. 将输入的全部样本作为根节点的样本集，对根节点进行 __步骤2__ 的操作。
2. 计算当前节点的信息熵，如果其信息熵为0，则 __停止操作__ ；否则，进入 __步骤3__ 。
3. 若所有特征都被祖先节点用来作为"分类特征"，则也 __停止操作__; 否则，进入 __步骤4__
4. 选取每一个祖先节点未用来作为“分类特征”的特征计算其信息增益。如果所有特征的信息增益都不大于0，则 __停止操作__ ；否则，选取信息增益最大的特征作为该节点的“分类特征”。
5. 假设分类特征X有N种特征值，则把该节点的样本集分为N份(对于第i份样本集中的特征保证X=xi{i=1,2,…,N})。将这N份样本集作为当前节点的N个子节点，遍历这N个子节点，分别对其进行 __步骤2__ 的操作 (递归操作)。

下面给出建树过程的图示：

```
                                      Y                  
                    {c1,c2,c3,c1,c1,c2,c1,c3,c2,c1,...}      g(Y|A)=0.43
                                      |                      g(Y|B)=1.24     
                                      |                      g(Y|C)=0.87
               __________________________________________________
       B=b1    |               B=b2 |                           |  B=b3
               |                    |                           |
              N1                    N2                          N3
         {c1,c2,...}           {c2,c2,...}                   {c1,c3,...}
g(N1|A)=0.42  |                  H(N2)=0                        |       g(N1|A)=0.35
g(N1|C)=0.13  |                                                 |       g(N1|C)=0.51
         ______________                          ______________________________________
   A=a1  |            | A=a2                     |           |            |           | 
         |            |                          |           |            |           |
         |            |                    C=c1  |      C=c2 |            | C=c3      | C=c4
        N4            N5                        N6          N7           N8           N9
  {c1,c1,...}     {c2,c2,...}                {c1,..}      {c1,..}      {c3,..}     {c1,c3,..}
   H(N4)=0          H(N5)=0                  H(N6)=0      H(N7)=0      H(N8)=0        |  H(N7)>0
                                                                                      |  g(N7|A)>0
                                                                                      |
                                                                                  __________
                                                                                  |        |  
                                                                            A=a1  |        |  A=a2
                                                                                  |        |
                                                                                 N10      N11
                                                                               {c1,..}  {c3,c3,c3,c1}
                                                                             H(N10)=0       H(N11)>0
```

## 测试
ID3算法的测试阶段是根据测试数据的特征值按照树节点的分类特征的分支一直往下走，走到叶子节点就能得到该测试数据的 __预测分类__ 。

算法步骤如下：

1. 按照 _分类特征_ 以及 _测试数据_ 的特征值，选取对应特征值的分支路径，一直往下走，直到叶子节点为止。
2. 到达叶子节点后，判断该叶子节点的信息熵是否为0。  
如果为0，说明该节点样本集中的样本分类一致，直接拿样本集中的一个样本的分类作为 __预测结果__ ；如果信息熵不为0，说明该节点样本集中夹杂了多个分类的样本，这时我们取样本集中样本数量最多的分类作为 __预测结果__。

下面给出例子：  
继续用上图的建好的决策树进行分类测试，假设有一个测试数据：T1={A=a1, B=b1, C=c1}, T2={A=a1,B=b2,C=c1}, T3={A=a2,B=b3,C=c4}。

对于T1的探索路径为：Y -> N1 -> N4。因为H(N4)=0，得出T1的 __预测分类__ 为c1。  
对于T2的探索路径为：Y -> N2。因为H(N4)=0，得出T2的 __预测分类__ 为c2。  
对于T3的探索路径为：Y -> N3 -> N9 -> N11。因为H(N11)>0，说明该节点的数据不纯，我们以该节点样本集中数量最多的分类作为T3的 __预测分类__ ，即为c3。


# 四、C4.5算法
C4.5算法与ID3算法的区别在于C4.5算法选择最优分类特征时采用信息增益率代替ID3的信息增益来评判分类的纯度，其余部分完全一样。

但如果同时存在多个特征求得的信息增益量相近，应该优先选择分支数最少的特征。这样分支数较多的特征会留到更底层的节点做划分，越底层的节点熵值越小，甚至到达阈值时就能对其进行剪枝，从而减少整个决策树的分支数量，提升效率以及泛化能力。

为了解决这个问题，C4.5采用**信息增益率**评估选取一个特征进行划分的优劣。

**信息增益率**公式如下：

$$
g(Y,X)=H(Y)-H(Y|X)
\\\\
S(Y,X) = - \sum^m_{i=1} \frac{|S^{(x_i)}|}{|Y|} \log \frac{|S^{(x_i)}|}{|Y|}
\\\\
GainRatio(Y,X)= \frac{g(Y,X)}{S(Y,X)}
$$

注意，S(Y,X)是熵函数，当分支数m越多，该函数的值就越大，信息增益率就会随之减少。这样C4.5算法就会在信息增益量相近的特征中，优先选取分支数最少的特征进行子集划分，从而解决ID3算法“更倾向于选择分支数较多的特征”的缺陷。

笔者的疑惑：笔者认为“信息增益率函数能使算法优先选择分支数少的特征做子集划分”这个观点不全对，观察一下S(Y,X)函数，即使分支数很多(即m很大)，但是有一个$|S^{(x_i)}|$趋向于1，剩余部分$|S^{(x_i)}|$则趋向于0时，函数S(Y,X)得到的值依然很小，这样一来信息增益率依然很大，会导致挑选了一个分支数非常多的特征进行子集划分。

# 五、CART算法
CART树是一棵二叉树，每个非叶子节点只划分两个子节点，每个节点上只会把一个特征的特征值分为两个子集。

比如“学历”特征，分别有3个特征值：{本科、硕士、博士}，CART有以下三种可能的分法：

* {本科,硕士}, {博士}
* {本科,博士}, {硕士}
* {硕士,博士}, {本科}

分别把两个特征值子集对应的样本分到两个子节点上，而选取哪一种划分方案，需要通过枚举每一种划分方案的__Gini系数__计算完毕后，选取__Gini系数__最小的划分方案为最优划分方案。	

CART算法采用Gini系数计算衡量分类不纯度。

$$
Gini(Y) = \sum^n_{i=1} \frac{|S_{c_i}|}{|Y|} (1 - \frac{|S_{c_i}|}{|Y|}) = 1 - \sum^n_{i=1} (\frac{|S_{c_i}|}{|Y|})^2
\\\\
0 \le Gini(Y) \le 1
$$

划分时，把节点Y的样本集划分为两个集合$S_1$和$S_2$，则对于样本集Y的不纯度计算函数为：

$$
GAIN(Y) = \frac{|S_1|}{Y} Gini(S_1) + \frac{|S_2|}{Y} Gini(S_2)
$$

而对训练集进行划分时，划分的规则是利用二叉树的表示形式，CART算法的开始是从根节点进行划分，对各个结点进行重复递归的过程：

1. 对于每个结点，计算各个剩余特征的最优分界点，并得到各个特征对应的最小Gini系数。注意：这里的剩余特征X可能在祖先节点处被用做过划分，它们两个节点对特征X的处理区别在于祖先节点中特征X的特征值子集比子节点关于特征X的特征值子集更大。
2. 选择Gini系数最小的特征按照最优分界点进行分割，分割成两个字节点。
3. 重复对此结点分割出来的两个子节点进行分割这一过程。
4. 节点划分的终止条件可以为：节点Gini系数下降到指定阈值、持续到叶结点样本个数极少（如少于5个）、树的深度达到指定阈值等。

## 六、剪枝
剪枝的作用除了简化决策树构造，提高算法效率之外，更重要的是“防止过拟合”。

防止决策树过拟合的方法就是：在训练时，生成的决策树节点细分程度不要过细，也就是所谓的“训练过度”。

可以设定一些指标的阈值，当决策树的某个指标到达预设的阈值时，即使该叶子节点的分类不纯也停止再细分。

这里给出一些常用的剪枝方案：

1. 节点混乱度(信息增益，信息增益率，Gini系数)下降到指定阈值时，停止对该节点再细分。
2. 限制树的最大深度。当决策树达到一定深度时，停止对该节点再细分。
3. 限制节点最小样本数。节点不断细分更多的子节点，层数越低的节点中包含的样本数越少。当节点样本数小于等于指定阈值，则停止对该节点细分。

## 七、随机森林
这个算法会生成N(可能会有几百棵以上）棵树，这样可以大大的减少单决策树带来的毛病，有点类似于三个臭皮匠等于一个诸葛亮的做法，虽然这几百棵决策树中的每一棵都很简单（相对于C4.5这种单决策树来说），但是他们组合起来确是很强大。

随机森林算法过程如下：

1. 以“有放回”的形式随机抽取目前提供的全部特征中的M个特征出来，然后对取出来的M个特征进行建树。假设原有特征为{A,B,C,D,E,F}，我们有放回的随机抽取4个特征，抽取的结果可能会是{A,C,D,D}、{B,B,B,A}, {A,C,E,F}等，然后根据抽取出来的4个特征用来构建一棵决策树。
2. 重复步骤1的方法构建N棵决策树，就形成了随机森林。
3. 当有测试数据传入进行分类预测时，把测试数据分别传入这N棵决策树中进行测试，从而得到N个预测结果。取这N个预测结果中出现频率最高的那个结果作为算法的最终预测值。
