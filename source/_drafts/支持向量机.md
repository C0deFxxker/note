<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

# 一、概述
支持向量机(Support Vector Machines, SVM)是 Vapnik 等人根据统计学习理论中结构风险最小化原则提出的，SVM能够尽量提高学习机的推广能力，即使由有限数据集得到的判别函数对独立的测试集仍能够得到较小的误差。此外，支持向量机是一个凸二次优化问题，能够保证找到极值解就是全局最优解。这个特点使支持向量机成为一种优秀的基于数据的机器学习算法。

# 二、算法思想
SVM算法的主要思想是：在N维空间中寻找一个最优划分超平面，使得用于分类训练的样本点根据自身类别完全划分在这个分割超平面的两侧，并且要求所有样本点到分割超平面的距离的最小值要最大。

下面用一个二维空间的图来解释这句话
![](http://www.blogjava.net/images/blogjava_net/zhenandaci/WindowsLiveWriter/SVMRefresh_9B92/image_thumb.png)

因为这里是二维空间，分割超平面就是一条直线，我们简称它是**分类线**。$H_1$、$H_2$ 分别为过各类中离分类线最近的数据样本且平行于分类线的直线，$H_1,H_2$之间的间隔我们称为**分类间隔**。我们的目标就是求得这条**最优分类线**的方程，所谓**最优分类线**就是要做到分类错误率为0，且分类间隔最大。

## 2.1 最优分类超平面
在n维空间中，定义一个超平面的一般形式为
$$
\omega^T \mathbf{x} + \mathbf{b} = 0
$$

假设单个训练样本的特征值为 $\mathbf{x}=[x_1, x_2, \cdots, x_n]$ 对应样本点坐标为 $(x_1, x_2, \cdots, x_n)$。我们可以通过以下线性判别函数判别样本点的分类情况：
$$
f(\mathbf{x}) = \omega^T \mathbf{x} + \mathbf{b}
$$

它把 $f(\mathbf{x}) > 0$ 和 $f(\mathbf{x}) < 0$ 的样本点分为了两类，所以我们定义分类标签为 $y$，$y$ 的取值为 -1 或 +1 分别表示不同的两个类别：
$$
y = sign(f(\mathbf{x})) = sign(\omega^T \mathbf{x} + \mathbf{b})
$$

因为y在训练样本中是已知的，因此我们可以用这样的函数表示样本点 $\mathbf{x}$ 到分割超平面的距离值：
$$
D(\mathbf{x}) = \frac{|f(\mathbf{x})|}{\|\omega\|} = \frac{y(\omega^T \mathbf{x} + \mathbf{b})}{\|\omega\|}
$$

我们可以线性变换 $f(x)$ 使得它对于所有的样本点都能满足： $|f(x)| \ge 1$，其中离分类超平面最近的样本点 $\mathbf{x}^\ast$ 满足 $ | f(\mathbf{x}^\ast) |=1 $。我们的距离函数 $D(\mathbf{x})$ 就得到如下不等式关系：
$$
\begin{aligned}
D(\mathbf{x}) \ge \frac{1}{\|\omega\|}
\end{aligned}
$$

只要找到 $\omega^\ast$ 使得 $\|\omega^\ast\| = \min{\|\omega\|}$，$\min D(\mathbf{x})$ 就能取得最大值，此时的 $f(\mathbf{x})=0$ 就是我们要找的**最优分割超平面**的一般形式。

求 $\frac{1}{\|\omega\|}$ 的最大值，相当于求 $\frac{1}{2}\|\omega\|^2$ 的最小值(乘上 $\frac{1}{2}$ 是为了后面求导方便)。

这样我们把整体目标定为：求 
$$
\begin{aligned}
& &\min_{\omega,b} \frac{1}{2} \omega^T\omega \\\\
&\text{s.t.}    &y_i(\mathbf{\omega} \cdot \mathbf{x}_i + \mathbf{b}) \ge 1
\end{aligned}
$$

它是一个只有不等式约束的优化问题，我们构建拉格朗日函数(参考 [拉格朗日对偶](http://blog.csdn.net/wangkr111/article/details/21170809) 以及 [拉格朗日乘子法和KKT条件](http://www.cnblogs.com/zhangchaoyang/articles/2726873.html))：
$$
\begin{aligned}
L(\omega, b, \mathbf{a}) &= \frac{1}{2} \omega^T\omega - \sum^n_{i=1} a_i \big( y_i(\omega^T \mathbf{x}_i + b) - 1 \big), &\text{其中拉格朗日乘子 } a_i \ge 0
\end{aligned}
$$

当 $\omega, b, \mathbf{a}$ 满足KKT条件时，拉格朗日对偶问题的解就是原问题的最优解，即 
$$
\displaystyle\min_{w,b} \displaystyle\max_{a} L(\omega, b, \mathbf{a}) 
= \displaystyle\max_{a} \displaystyle\min_{w,b} L(\omega, b, \mathbf{a}) 
= \min \frac{1}{2} \omega^T\omega
$$

以下列出KKT条件：
$$
\begin{cases}
\frac{\partial L(\omega, b, \mathbf{a})}{\partial \omega} = 0\\\\
\frac{\partial L(\omega, b, \mathbf{a})}{\partial b} = 0\\\\
a_i \big( y_i(\omega^T \mathbf{x}_i + b) - 1 \big) = 0\\\\
y_i(\omega^T \mathbf{x}_i + b) \ge 1\\\\
a_i \ge 0
\end{cases}
$$

因为该式子的最小值出现在各个变量偏求导等于0处：
$$
\begin{aligned}
\frac{\partial L(\omega, b, \mathbf{a})}{\partial \omega} = 0 &\implies \omega = \sum\_{i=1}^n a_i y_i \mathbf{x}_i\\\\
\frac{\partial L(\omega, b, \mathbf{a})}{\partial b} = 0 &\implies \sum\_{i=1}^n a_i y_i = 0
\end{aligned}
$$

将上述二式代入目标函数：
$$
\begin{aligned}
L(\omega, b, \mathbf{a}) &= \frac{1}{2} \omega^T\omega - \sum^n_{i=1} a_i \big( y_i(\omega^T \mathbf{x}_i + b) - 1 \big)\\\\
&= \frac{1}{2} \bigg( \sum\_{i=1}^n a_i y_i \mathbf{x}_i^T \bigg) \bigg( \sum\_{j=1}^n a_j y_j \mathbf{x}_j \bigg) - \sum^n\_{i=1} a_i \bigg( y_i(\sum\_{j=1}^n a_j y_j \mathbf{x}_j^T) \cdot \mathbf{x}_i + b) - 1 \bigg)\\\\
&= \frac{1}{2}  \sum\_{i,j=1}^n a_i a_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - \sum\_{i,j=1}^n a_i a_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - b \sum\_{i=1}^n a_i y_i + \sum\_{i=1}^n a_i\\\\
&= \sum\_{i=1}^n a_i - \frac{1}{2}  \sum\_{i,j=1}^n a_i a_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j
\end{aligned}
$$

来到这里，这个最优化问题已经跟 $\omega$ 以及 $b$ 无关了，可以把上述问题转化为如下凸二次规划寻优的对偶问题：
$$
\begin{cases}
\max \displaystyle\sum\_{i=1}^n a_i - \frac{1}{2}  \sum\_{i,j=1}^n a_i a_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j\\\\
a_i \ge 0\\\\
\displaystyle\sum\_{i=1}^n a_i y_i = 0
\end{cases}
$$

若找到一个最优解 $\mathbf{a}^\star$，我们就可以通过如下式子求出 $\omega^\star$:
$$
\omega^\star = \sum\_{i=1}^n a^\star_i y_i \mathbf{x}_i
$$

求完 $\omega^\star$ 之后我们来求 $b^\star$。分布在决策边界上的样本集，即为**支持向量**，只有它们会对决策边界产生影响。因此，我们只要利用其中一个支持向量 $\mathbf{x}_i$ 它们必定满足下列等式，从而求得 $b^\star$:
$$
\begin{aligned}
&y_i(\omega^T \mathbf{x}_i + b^\star) = 1, &\text{其中 } \mathbf{x}_i \text{ 为支持向量}
\end{aligned}
$$

在拉格朗日对偶问题求解时，若 $y_i(\omega^T \mathbf{x}_i + b) > 1$ 时，对偶函数 $\displaystyle\min\_{w,b} \displaystyle\max\_{a} L(\omega, b, \mathbf{a})$ 会调整系数 $a_i = 0$ 使得结果最大，所以对于 $a_i \ne 0$ 对应的样本就是我们要找的**支持向量**，这些样本满足 $y_i(\omega^T \mathbf{x}_i + b) = 1$。

## 2.2 防止过拟合
有些时候，可能会因为数据有误差或者是为了防止数据过拟合而允许分类器容忍一定程度的误差，我们为原来的约束条件 $|f(\mathbf{x})| \ge 1$ 添加一个松弛变量:
$$
\begin{aligned}
|f(\mathbf{x})| &\ge 1 - \xi,     &\text{其中 }0 \le \xi\\\\
\implies y_i( \omega^T \mathbf{x} + \mathbf{b} ) &\ge 1 - \xi
\end{aligned}
$$

添加松弛变量后效果如下图:
![](http://images.cnitblog.com/blog/458371/201212/31223704-0758d0b56ff7413985d4dacd3b0431d5.jpg)

从图中我们可以看出，当 $0 \le \xi \lt 1$ 时，分类器依然能正确分类；当 $1 \lt \xi$ 时，分类器得到的分类结果会有错误。

我们在能容忍分类器存在一定错误的情况下，希望总体误差很小，并且使得分类间隔足够大，我们把目标函数改为求：
$$
\begin{aligned}
&\displaystyle\min_{\omega,b,\xi} \bigg( \frac{1}{2} \omega^T\omega + C \sum^n_{i=1} \xi_i \bigg), &\text{其中 } C > 0 \text{ 且 } \xi_i \ge 0\\\\
&s.t. \space\space\space\space y( \omega^T \mathbf{x} + \mathbf{b} ) \ge 1 - \xi\\\\
&\space\space\space\space\space\space\space\space\space\space \xi_i \ge 0
\end{aligned}
$$

其中 $C$ 是我们预设的参数，它的大小影响误差值 $\xi$ 所占的权重。当 $C=+\infty$ 只要有一个 $\xi_i \ne 0$ 则整个目标函数就等于无穷大，这意味着分类器不能容忍有一丝误差（相当于回归到2.1节的目标函数：$\displaystyle\min_{\omega,b} \frac{1}{2} \omega^T\omega$）。

我们依然使用“拉格朗日乘子法”解决带约束的最优化问题：
$$
\begin{aligned}
L(\omega, b, \xi, \mathbf{a}, \mathbf{u}) &= \frac{1}{2} \omega^T\omega + C \sum^n_{i=1} \xi_i - \sum^n_{i=1} a_i \big( y_i(\omega^T \mathbf{x}_i + b) - 1 + \xi_i \big) - \sum\_{i=1}^n u_i \xi_i, &\text{其中拉格朗日乘子 } a_i, u_i \ge 0
\end{aligned}
$$

因为该式子的最小值出现在各个变量偏求导等于0处：
$$
\begin{aligned}
\frac{\partial L(\omega, b, \xi, \mathbf{a}, \mathbf{u})}{\partial \omega} = 0 &\implies \omega = \sum\_{i=1}^n a_i y_i \mathbf{x}_i\\\\
\frac{\partial L(\omega, b, \xi, \mathbf{a}, \mathbf{u})}{\partial b} = 0 &\implies \sum\_{i=1}^n a_i y_i = 0\\\\
\frac{\partial L(\omega, b, \xi, \mathbf{a}, \mathbf{u})}{\partial \xi_i} = 0 &\implies C - a_i - u_i = 0
\end{aligned}
$$

我们将上面三个式子代入目标函数：
$$
\begin{aligned}
L(\omega, b, \xi, \mathbf{a}, \mathbf{u}) &= \frac{1}{2} \omega^T\omega + C \sum^n_{i=1} \xi_i - \sum^n_{i=1} a_i \big( y_i(\omega^T \mathbf{x}_i + b) - 1 + \xi_i \big) - \sum\_{i=1}^n u_i \xi_i\\\\
&= \frac{1}{2} \bigg( \sum\_{i=1}^n a_i y_i \mathbf{x}_i^T \bigg) \bigg( \sum\_{j=1}^n a_j y_j \mathbf{x}_j \bigg) + C \sum^n\_{i=1} \xi_i - \sum^n\_{i=1} a_i \bigg( y_i(\sum\_{j=1}^n a_j y_j \mathbf{x}_j^T) \cdot \mathbf{x}_i + b) - 1 + \xi_i \bigg) - \sum\_{i=1}^n (C - a_i) \xi_i\\\\
&= \frac{1}{2}  \sum\_{i,j=1}^n a_i a_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - \sum\_{i,j=1}^n a_i a_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j - b \sum\_{i=1}^n a_i y_i + \sum\_{i=1}^n a_i\\\\
&= \sum\_{i=1}^n a_i - \frac{1}{2}  \sum\_{i,j=1}^n a_i a_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j
\end{aligned}
$$

其中通过 $C - a_i - u_i = 0$ 推导出约束条件：
$$
C \ge a_i \ge 0
$$

来到这里发现式子跟2.1节的一模一样，它们的区别只在于约束条件：
$$
\begin{aligned}
&C \ge a_i \ge 0, &\sum\_{i=1}^n a_i y_i = 0
\end{aligned}
$$
